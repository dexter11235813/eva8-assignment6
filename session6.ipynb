{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "batch_size = 128 \n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.mnist_model = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(8, 12, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(12, 16, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(16, 32, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(32, 24, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(24, 20, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(20,16,3,bias = False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(16,16,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 10, 3),\n",
    "            nn.AvgPool2d(3)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mnist_model(x)\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "        transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "    ])), batch_size=batch_size, shuffle=True,**kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "        transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "    ])), batch_size=batch_size, shuffle=True,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    train_correct = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for _, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss = F.nll_loss(output, target)\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for param in optimizer.param_groups:\n",
    "          lr = param['lr']\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    \n",
    "    #return 100.0 * train_correct/len(train_loader.dataset)\n",
    "    print (f'\\nTrain set: Average loss: {train_loss}, Train Accuracy: {train_correct}/{len(train_loader.dataset)} ({100. * train_correct / len(train_loader.dataset)}), learning rate : {lr}')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "   # return  1.0 * correct / len(test_loader.dataset)\n",
    "\n",
    "    print (f'\\nTest set: Average loss: {test_loss}, Test Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 28, 28]              72\n",
      "              ReLU-2            [-1, 8, 28, 28]               0\n",
      "       BatchNorm2d-3            [-1, 8, 28, 28]              16\n",
      "           Dropout-4            [-1, 8, 28, 28]               0\n",
      "            Conv2d-5           [-1, 12, 28, 28]             864\n",
      "              ReLU-6           [-1, 12, 28, 28]               0\n",
      "       BatchNorm2d-7           [-1, 12, 28, 28]              24\n",
      "           Dropout-8           [-1, 12, 28, 28]               0\n",
      "            Conv2d-9           [-1, 16, 28, 28]           1,728\n",
      "             ReLU-10           [-1, 16, 28, 28]               0\n",
      "      BatchNorm2d-11           [-1, 16, 28, 28]              32\n",
      "          Dropout-12           [-1, 16, 28, 28]               0\n",
      "           Conv2d-13           [-1, 32, 28, 28]           4,608\n",
      "             ReLU-14           [-1, 32, 28, 28]               0\n",
      "      BatchNorm2d-15           [-1, 32, 28, 28]              64\n",
      "        MaxPool2d-16           [-1, 32, 14, 14]               0\n",
      "          Dropout-17           [-1, 32, 14, 14]               0\n",
      "           Conv2d-18           [-1, 24, 14, 14]             768\n",
      "             ReLU-19           [-1, 24, 14, 14]               0\n",
      "      BatchNorm2d-20           [-1, 24, 14, 14]              48\n",
      "          Dropout-21           [-1, 24, 14, 14]               0\n",
      "           Conv2d-22           [-1, 20, 14, 14]           4,320\n",
      "             ReLU-23           [-1, 20, 14, 14]               0\n",
      "      BatchNorm2d-24           [-1, 20, 14, 14]              40\n",
      "          Dropout-25           [-1, 20, 14, 14]               0\n",
      "           Conv2d-26           [-1, 16, 14, 14]           2,880\n",
      "             ReLU-27           [-1, 16, 14, 14]               0\n",
      "      BatchNorm2d-28           [-1, 16, 14, 14]              32\n",
      "        MaxPool2d-29             [-1, 16, 7, 7]               0\n",
      "          Dropout-30             [-1, 16, 7, 7]               0\n",
      "           Conv2d-31             [-1, 16, 5, 5]           2,320\n",
      "             ReLU-32             [-1, 16, 5, 5]               0\n",
      "      BatchNorm2d-33             [-1, 16, 5, 5]              32\n",
      "           Conv2d-34             [-1, 10, 3, 3]           1,450\n",
      "        AvgPool2d-35             [-1, 10, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 19,298\n",
      "Trainable params: 19,298\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.89\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 1.96\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5133/3073436661.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MNISTNet().to(device)\n",
    "summary(model, input_size=(1, 28, 28))\n",
    "\n",
    "init_lr = 0.03\n",
    "optimizer = optim.Adam(model.parameters(), lr = init_lr)\n",
    "def adjust_lr(optimizer, epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "      init_lr = param_group['lr']\n",
    "    lr = max(round(init_lr * 1/(1 + np.pi/50 * epoch), 10), 0.0005)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 / 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5133/3073436661.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.001541243982501328, Train Accuracy: 56251/60000 (93.75166666666667), learning rate : 0.03\n",
      "\n",
      "Test set: Average loss: 0.07403078970760107, Test Accuracy: 9770/10000 (97.7)\n",
      "EPOCH 2 / 20\n",
      "\n",
      "Train set: Average loss: 0.0006116885924711823, Train Accuracy: 58524/60000 (97.54), learning rate : 0.0282264781\n",
      "\n",
      "Test set: Average loss: 0.052058789293468, Test Accuracy: 9849/10000 (98.49)\n",
      "EPOCH 3 / 20\n",
      "\n",
      "Train set: Average loss: 0.0004842551425099373, Train Accuracy: 58854/60000 (98.09), learning rate : 0.0250754092\n",
      "\n",
      "Test set: Average loss: 0.0389847578048706, Test Accuracy: 9871/10000 (98.71)\n",
      "EPOCH 4 / 20\n",
      "\n",
      "Train set: Average loss: 0.00040306031587533653, Train Accuracy: 59010/60000 (98.35), learning rate : 0.0210984459\n",
      "\n",
      "Test set: Average loss: 0.027241934108361603, Test Accuracy: 9909/10000 (99.09)\n",
      "EPOCH 5 / 20\n",
      "\n",
      "Train set: Average loss: 0.000352283037500456, Train Accuracy: 59146/60000 (98.57666666666667), learning rate : 0.0168608517\n",
      "\n",
      "Test set: Average loss: 0.02854687395170331, Test Accuracy: 9904/10000 (99.04)\n",
      "EPOCH 6 / 20\n",
      "\n",
      "Train set: Average loss: 0.0003047494392376393, Train Accuracy: 59232/60000 (98.72), learning rate : 0.0128301433\n",
      "\n",
      "Test set: Average loss: 0.0205464142665267, Test Accuracy: 9935/10000 (99.35)\n",
      "EPOCH 7 / 20\n",
      "\n",
      "Train set: Average loss: 0.000265876529738307, Train Accuracy: 59354/60000 (98.92333333333333), learning rate : 0.0093175207\n",
      "\n",
      "Test set: Average loss: 0.01997793157864362, Test Accuracy: 9932/10000 (99.32)\n",
      "EPOCH 8 / 20\n",
      "\n",
      "Train set: Average loss: 0.00022522397921420634, Train Accuracy: 59461/60000 (99.10166666666667), learning rate : 0.006471296\n",
      "\n",
      "Test set: Average loss: 0.016697057465836405, Test Accuracy: 9947/10000 (99.47)\n",
      "EPOCH 9 / 20\n",
      "\n",
      "Train set: Average loss: 0.00020969012985005975, Train Accuracy: 59501/60000 (99.16833333333334), learning rate : 0.0043065752\n",
      "\n",
      "Test set: Average loss: 0.01643691094685346, Test Accuracy: 9943/10000 (99.43)\n",
      "EPOCH 10 / 20\n",
      "\n",
      "Train set: Average loss: 0.0001864134828792885, Train Accuracy: 59543/60000 (99.23833333333333), learning rate : 0.0027509498\n",
      "\n",
      "Test set: Average loss: 0.016933483577147128, Test Accuracy: 9947/10000 (99.47)\n",
      "EPOCH 11 / 20\n",
      "\n",
      "Train set: Average loss: 0.00017929250316228718, Train Accuracy: 59553/60000 (99.255), learning rate : 0.0016894421\n",
      "\n",
      "Test set: Average loss: 0.015878893764317035, Test Accuracy: 9951/10000 (99.51)\n",
      "EPOCH 12 / 20\n",
      "\n",
      "Train set: Average loss: 0.00016430924006272107, Train Accuracy: 59594/60000 (99.32333333333334), learning rate : 0.0009989899\n",
      "\n",
      "Test set: Average loss: 0.014997824397636578, Test Accuracy: 9950/10000 (99.5)\n",
      "EPOCH 13 / 20\n",
      "\n",
      "Train set: Average loss: 0.0001597372756805271, Train Accuracy: 59606/60000 (99.34333333333333), learning rate : 0.0005695553\n",
      "\n",
      "Test set: Average loss: 0.015060545969987288, Test Accuracy: 9946/10000 (99.46)\n",
      "EPOCH 14 / 20\n",
      "\n",
      "Train set: Average loss: 0.00015216968313325197, Train Accuracy: 59618/60000 (99.36333333333333), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.014689279747952241, Test Accuracy: 9948/10000 (99.48)\n",
      "EPOCH 15 / 20\n",
      "\n",
      "Train set: Average loss: 0.00015684323443565518, Train Accuracy: 59627/60000 (99.37833333333333), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.014791468676924706, Test Accuracy: 9949/10000 (99.49)\n",
      "EPOCH 16 / 20\n",
      "\n",
      "Train set: Average loss: 0.0001560943783260882, Train Accuracy: 59601/60000 (99.335), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.014654756718128919, Test Accuracy: 9951/10000 (99.51)\n",
      "EPOCH 17 / 20\n",
      "\n",
      "Train set: Average loss: 0.00015577928570564836, Train Accuracy: 59625/60000 (99.375), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.014942640182375908, Test Accuracy: 9950/10000 (99.5)\n",
      "EPOCH 18 / 20\n",
      "\n",
      "Train set: Average loss: 0.00014829456631559879, Train Accuracy: 59609/60000 (99.34833333333333), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.014591951730754227, Test Accuracy: 9948/10000 (99.48)\n",
      "EPOCH 19 / 20\n",
      "\n",
      "Train set: Average loss: 0.00015196128515526652, Train Accuracy: 59624/60000 (99.37333333333333), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.014888232602644711, Test Accuracy: 9949/10000 (99.49)\n",
      "EPOCH 20 / 20\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(f'EPOCH {epoch + 1} / {20}')\n",
    "    adjust_lr(optimizer,epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
